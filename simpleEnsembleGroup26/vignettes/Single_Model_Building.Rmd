---
title: "Single Model Building with & without Parameter Tuning & Cross-Validation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Single Model Building with & without Parameter Tuning & Cross-Validation}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r}
library(simpleEnsembleGroup26)
```

# Instructions for Single Model Building

When building a single model using the `unifiedModelingToolkit` function, it's important to follow these guidelines for the function to work correctly:

-   **Model Selection**: Pass a list containing only one model name to the `models` parameter. You can choose from the following models: `list("Lasso_regression", "lasso", "ridge", "elastic_net", "random_forest")`.
-   **Problem Type Detection**: The function automatically detects the problem type (classification or regression) based on the type of the response variable `y`. For binary classification, `y` should be a binary factor or numeric vector with two levels. For regression, `y` should be a continuous variable.
-   **Ensemble and Bagging**: Ensure both `ensemble` and `bagging` parameters are set to `FALSE` when aiming to build a single model. This is crucial as enabling either of these parameters would activate the function's bagging or ensemble learning features, which are not intended for single model scenarios.

## Example 1: Building a Lasso Regression Model for Classification - without CV & Parameter tuning.

This example demonstrates how to prepare your data and utilize the `unifiedModelingToolkit` function to build a Lasso regression model for a binary classification problem, without parameter tuning and cross-validation.

### 1.1 Prepare Data

```{r}
data(iris)
# Extract features (X) and target variable (y)
X <- iris[, -5]  # Exclude the last column which contains the species names
y <- ifelse(iris$Species == "setosa", 1, 0)  # Convert species to binary labels (1 for setosa, 0 for non-setosa)

n = nrow(X)
train_idx <- sample(1:n, 0.7 * n)
X_train <- X[train_idx, ]
y_train <- y[train_idx]
X_test <- X[-train_idx, ]
y_test <- y[-train_idx]
```

### 1.2 Fit a model

```{r}
# Define models list with one model for classification
models <- list("lasso")

# Run unifiedModelingToolkit for single model building
results <- unifiedModelingToolkit(X = X_train, y = y_train, models = models, parameter_tuning = FALSE)

# The function will return a list containing the model, predictions, and metrics
# Here's how to access the model's performance metrics:

print(names(results))
results$metrics
```
### 1.3 Make predicitons on test data
```{r}
model = results$model
y_predictions =  make_predictions(model, X_test)$predictions
calculate_binary_classification_metrics(y_test, y_predictions)
```

## Example 2: Building a Random Forest Model for Classification - with CV & Parameter tuning.

This example demonstrates how to prepare your data and utilize the `unifiedModelingToolkit` function to build a Random Forest model for a binary classification problem, without parameter tuning and cross-validation.

### 2.1 Prepare Data

```{r}
data_builder = function(problem_type, n = 100) {
  set.seed(123)
  data = NULL
  if ('classification' == problem_type){
    data <- data.frame(
      feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
      feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
      feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
      feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
      feature_num5 = rnorm(n, mean = 9, sd = 17),  # Numerical feature
      feature_cat = sample(c("Category1", "Category2", "Category3"), n, replace = TRUE),  # Categorical feature
      target = sample(0:1, n, replace = TRUE)  # Binary target variable
    )
  } else {
    data <- data.frame(
      feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
      feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
      feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
      feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
      feature_cat = sample(c("Category1", "Category2", "Category3"), n, replace = TRUE),  # Categorical feature
      target = rnorm(sample(0:1, n, replace = TRUE))  # reg target variable
    )
  }
  X <- data[, -which(names(data) == "target")]
  y <- data[["target"]]
  return(list(X = X, y = y))
}
data = data_builder('classification')
X = data$X
y = data$y
```


### 2.2 Fit a model

```{r}
models <- list("random_forest")
model_params = list(random_forest = list(ntree = 200, mtry = c(1, 5 , 10), improve = 0.025))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_params, parameter_tuning = TRUE)
print(names(results))
results$metrics
```
## Example 2: Building a Random Forest Model for Regression - with CV & Parameter tuning.
### Let us also choose to top k predictors by specifying either k or feature_selection_method
```{r}
data = data_builder('regression')
X = data$X
y = data$y
models <- list("linear_regression")
model_params = list(linear_regression = list())
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_params, parameter_tuning = TRUE, scale_data = TRUE, k = 4)
print(names(results))
results$metrics
```

