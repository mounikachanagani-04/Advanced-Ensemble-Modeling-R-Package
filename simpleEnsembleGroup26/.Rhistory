feature_nu7 = rnorm(n, mean = 0.11, sd = 1.2),  # Numerical feature
feature_nu8 = rnorm(n, mean = 3.11, sd = 7.2),  # Numerical feature
target = rnorm(sample(0:1, n, replace = TRUE))  # Binary target variable
)
}
X <- data[, -which(names(data) == "target")]
y <- data[["target"]]
return(list(X = X, y = y))
}
# linear_regression ------
data = data_builder('regression')
dim(data.frame(data))
X = data$X
y = data$y
model_type = "linear_regression"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=0.01))
results <- unifiedModelingToolkit(X = X, y = y, models = models)
results
library(assertthat)
library(simpleEnsembleGroup26)
data_builder = function(problem_type) {
set.seed(123)
n = 5
data = NULL
if ('classification' == problem_type){
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_num5 = rnorm(n, mean = 0.5, sd = 2),  # Numerical feature
feature_nu6 = rnorm(n, mean = 0.77, sd = 5),  # Numerical feature
feature_nu7 = rnorm(n, mean = 0.11, sd = 1.2),  # Numerical feature
feature_nu8 = rnorm(n, mean = 3.11, sd = 7.2),  # Numerical feature
target = sample(0:1, n, replace = TRUE)  # Binary target variable
)
} else {
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_num5 = rnorm(n, mean = 0.5, sd = 2),  # Numerical feature
feature_nu6 = rnorm(n, mean = 0.77, sd = 5),  # Numerical feature
feature_nu7 = rnorm(n, mean = 0.11, sd = 1.2),  # Numerical feature
feature_nu8 = rnorm(n, mean = 3.11, sd = 7.2),  # Numerical feature
target = rnorm(sample(0:1, n, replace = TRUE))  # Binary target variable
)
}
X <- data[, -which(names(data) == "target")]
y <- data[["target"]]
return(list(X = X, y = y))
}
# linear_regression ------
data = data_builder('regression')
dim(data.frame(data))
X = data$X
y = data$y
model_type = "linear_regression"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=0.01))
results <- unifiedModelingToolkit(X = X, y = y, models = models)
assert_that(!is.null(results$model))
print(results$predictions[1:5])
assert_that(!('mse' %in% names(results$metrics)), msg = 'MSE is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 5)
# logistic regression ------
data = data_builder('classification')
X = data$X
y = data$y
model_type = "logistic_regression"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=0.01))
results <- unifiedModelingToolkit(X = X, y = y, models = models)
assert_that(!is.null(results$model))
assert_that(length(unique(results$predictions)) == 2)
assert_that(('accuracy' %in% names(results$metrics)), msg = 'Accuracy is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 5)
library(assertthat)
library(simpleEnsembleGroup26)
data_builder = function(problem_type) {
set.seed(123)
n = 100
data = NULL
if ('classification' == problem_type){
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_num5 = rnorm(n, mean = 0.5, sd = 2),  # Numerical feature
feature_nu6 = rnorm(n, mean = 0.77, sd = 5),  # Numerical feature
feature_nu7 = rnorm(n, mean = 0.11, sd = 1.2),  # Numerical feature
feature_nu8 = rnorm(n, mean = 3.11, sd = 7.2),  # Numerical feature
target = sample(0:1, n, replace = TRUE)  # Binary target variable
)
} else {
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_num5 = rnorm(n, mean = 0.5, sd = 2),  # Numerical feature
feature_nu6 = rnorm(n, mean = 0.77, sd = 5),  # Numerical feature
feature_nu7 = rnorm(n, mean = 0.11, sd = 1.2),  # Numerical feature
feature_nu8 = rnorm(n, mean = 3.11, sd = 7.2),  # Numerical feature
target = rnorm(sample(0:1, n, replace = TRUE))  # Binary target variable
)
}
X <- data[, -which(names(data) == "target")]
y <- data[["target"]]
return(list(X = X, y = y))
}
# linear_regression ------
data = data_builder('regression')
dim(data.frame(data))
X = data$X
y = data$y
model_type = "lasso"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=list(0.01, 0.05, 0.07)))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params)
assert_that(!is.null(results$model))
print(results$predictions[1:5])
assert_that(!('mse' %in% names(results$metrics)), msg = 'MSE is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 6)
# logistic regression ------
data = data_builder('classification')
X = data$X
y = data$y
model_type = "logistic_regression"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=0.01))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params)
assert_that(!is.null(results$model))
assert_that(length(unique(results$predictions)) == 2)
assert_that(('accuracy' %in% names(results$metrics)), msg = 'Accuracy is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 6)
library(assertthat)
library(simpleEnsembleGroup26)
data_builder = function(problem_type) {
set.seed(123)
n = 100
data = NULL
if ('classification' == problem_type){
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_num5 = rnorm(n, mean = 0.5, sd = 2),  # Numerical feature
feature_nu6 = rnorm(n, mean = 0.77, sd = 5),  # Numerical feature
feature_nu7 = rnorm(n, mean = 0.11, sd = 1.2),  # Numerical feature
feature_nu8 = rnorm(n, mean = 3.11, sd = 7.2),  # Numerical feature
target = sample(0:1, n, replace = TRUE)  # Binary target variable
)
} else {
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_num5 = rnorm(n, mean = 0.5, sd = 2),  # Numerical feature
feature_nu6 = rnorm(n, mean = 0.77, sd = 5),  # Numerical feature
feature_nu7 = rnorm(n, mean = 0.11, sd = 1.2),  # Numerical feature
feature_nu8 = rnorm(n, mean = 3.11, sd = 7.2),  # Numerical feature
target = rnorm(sample(0:1, n, replace = TRUE))  # Binary target variable
)
}
X <- data[, -which(names(data) == "target")]
y <- data[["target"]]
return(list(X = X, y = y))
}
# lasso_regression ------
data = data_builder('regression')
dim(data.frame(data))
X = data$X
y = data$y
model_type = "lasso"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=list(0.01, 0.05, 0.07)))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params)
assert_that(!is.null(results$model))
print(results$predictions[1:5])
assert_that(!('mse' %in% names(results$metrics)), msg = 'MSE is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 6)
# ridge regression ------
data = data_builder('classification')
X = data$X
y = data$y
model_type = "ridge"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=list(0.01, 0.05, 0.07)))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params)
assert_that(!is.null(results$model))
assert_that(length(unique(results$predictions)) == 2)
assert_that(('accuracy' %in% names(results$metrics)), msg = 'Accuracy is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 6)
# elasticnet regression ------
data = data_builder('regression')
X = data$X
y = data$y
model_type = "elasticnet"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=list(0.01, 0.05, 0.07)))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params)
library(assertthat)
library(simpleEnsembleGroup26)
data_builder = function(problem_type) {
set.seed(123)
n = 100
data = NULL
if ('classification' == problem_type){
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_num5 = rnorm(n, mean = 0.5, sd = 2),  # Numerical feature
feature_nu6 = rnorm(n, mean = 0.77, sd = 5),  # Numerical feature
feature_nu7 = rnorm(n, mean = 0.11, sd = 1.2),  # Numerical feature
feature_nu8 = rnorm(n, mean = 3.11, sd = 7.2),  # Numerical feature
target = sample(0:1, n, replace = TRUE)  # Binary target variable
)
} else {
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_num5 = rnorm(n, mean = 0.5, sd = 2),  # Numerical feature
feature_nu6 = rnorm(n, mean = 0.77, sd = 5),  # Numerical feature
feature_nu7 = rnorm(n, mean = 0.11, sd = 1.2),  # Numerical feature
feature_nu8 = rnorm(n, mean = 3.11, sd = 7.2),  # Numerical feature
target = rnorm(sample(0:1, n, replace = TRUE))  # Binary target variable
)
}
X <- data[, -which(names(data) == "target")]
y <- data[["target"]]
return(list(X = X, y = y))
}
# lasso_regression ------
data = data_builder('regression')
dim(data.frame(data))
X = data$X
y = data$y
model_type = "lasso"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=list(0.01, 0.05, 0.07)))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params)
assert_that(!is.null(results$model))
print(results$predictions[1:5])
assert_that(!('mse' %in% names(results$metrics)), msg = 'MSE is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 6)
# ridge regression ------
data = data_builder('classification')
X = data$X
y = data$y
model_type = "ridge"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=list(0.01, 0.05, 0.07)))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params)
assert_that(!is.null(results$model))
assert_that(length(unique(results$predictions)) == 2)
assert_that(('accuracy' %in% names(results$metrics)), msg = 'Accuracy is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 6)
# elasticnet regression ------
data = data_builder('regression')
X = data$X
y = data$y
model_type = "elastic_net"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=list(0.01, 0.05, 0.07)))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params)
assert_that(!is.null(results$model))
assert_that(length(unique(results$predictions)) == 2)
library(assertthat)
library(simpleEnsembleGroup26)
data_builder = function(problem_type) {
set.seed(123)
n = 100
data = NULL
if ('classification' == problem_type){
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_num5 = rnorm(n, mean = 0.5, sd = 2),  # Numerical feature
feature_nu6 = rnorm(n, mean = 0.77, sd = 5),  # Numerical feature
feature_nu7 = rnorm(n, mean = 0.11, sd = 1.2),  # Numerical feature
feature_nu8 = rnorm(n, mean = 3.11, sd = 7.2),  # Numerical feature
target = sample(0:1, n, replace = TRUE)  # Binary target variable
)
} else {
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_num5 = rnorm(n, mean = 0.5, sd = 2),  # Numerical feature
feature_nu6 = rnorm(n, mean = 0.77, sd = 5),  # Numerical feature
feature_nu7 = rnorm(n, mean = 0.11, sd = 1.2),  # Numerical feature
feature_nu8 = rnorm(n, mean = 3.11, sd = 7.2),  # Numerical feature
target = rnorm(sample(0:1, n, replace = TRUE))  # Binary target variable
)
}
X <- data[, -which(names(data) == "target")]
y <- data[["target"]]
return(list(X = X, y = y))
}
# lasso_regression ------
data = data_builder('regression')
dim(data.frame(data))
X = data$X
y = data$y
model_type = "lasso"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=list(0.01, 0.05, 0.07)))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params)
assert_that(!is.null(results$model))
print(results$predictions[1:5])
assert_that(!('mse' %in% names(results$metrics)), msg = 'MSE is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 6)
# ridge regression ------
data = data_builder('classification')
X = data$X
y = data$y
model_type = "ridge"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=list(0.01, 0.05, 0.07)))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params)
assert_that(!is.null(results$model))
assert_that(length(unique(results$predictions)) == 2)
assert_that(('accuracy' %in% names(results$metrics)), msg = 'Accuracy is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 6)
# elasticnet regression ------
data = data_builder('classification')
X = data$X
y = data$y
model_type = "elastic_net"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=list(0.01, 0.05, 0.07)))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params)
assert_that(!is.null(results$model))
assert_that(length(unique(results$predictions)) == 2)
assert_that(('accuracy' %in% names(results$metrics)), msg = 'Accuracy is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 6)
?data_builder
?data_builder
library(simpleEnsembleGroup26)
?data_builder
data_builder
?unifiedModelingToolkit
data_builder = function(problem_type) {
set.seed(123)
n = 100
data = NULL
if ('classification' == problem_type){
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_cat = sample(c("Category1", "Category2", "Category3"), n, replace = TRUE),  # Categorical feature
target = sample(0:1, n, replace = TRUE)  # Binary target variable
)
} else {
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_cat = sample(c("Category1", "Category2", "Category3"), n, replace = TRUE),  # Categorical feature
target = rnorm(sample(0:1, n, replace = TRUE))  # reg target variable
)
}
X <- data[, -which(names(data) == "target")]
y <- data[["target"]]
return(list(X = X, y = y))
}
print('# --------------------------------- Single MODEL BUILDING ---------------------------------')
# linear_regression ------
data = data_builder('regression')
X = data$X
y = data$y
model_type = "linear_regression"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=0.01))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params,
bagging = FALSE, bagging_R = 100, ensemble = FALSE, feature_selection_method = NULL,
k = NULL, drop_missing_records = FALSE, fill_missing_method = "mean",
scale_data = FALSE, remove_outliers = FALSE, seed = 123, parameter_tuning = FALSE, cv_folds = 10, ensemble_combine_method = NULL)
assert_that(!is.null(results$model))
print(results$predictions[1:10])
assert_that(!('mse' %in% names(results$metrics)), msg = 'MSE is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 6)
data_builder = function(problem_type) {
set.seed(123)
n = 100
data = NULL
if ('classification' == problem_type){
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_cat = sample(c("Category1", "Category2", "Category3"), n, replace = TRUE),  # Categorical feature
target = sample(0:1, n, replace = TRUE)  # Binary target variable
)
} else {
data <- data.frame(
feature_num1 = rnorm(n, mean = 5, sd = 2),  # Numerical feature
feature_num2 = rnorm(n, mean = 55, sd = 42),  # Numerical feature
feature_num3 = rnorm(n, mean = 544, sd = 52),  # Numerical feature
feature_num4 = rnorm(n, mean = 59, sd = 16),  # Numerical feature
feature_cat = sample(c("Category1", "Category2", "Category3"), n, replace = TRUE),  # Categorical feature
target = rnorm(sample(0:1, n, replace = TRUE))  # reg target variable
)
}
X <- data[, -which(names(data) == "target")]
y <- data[["target"]]
return(list(X = X, y = y))
}
print('# --------------------------------- Single MODEL BUILDING ---------------------------------')
# linear_regression ------
data = data_builder('regression')
X = data$X
y = data$y
model_type = "linear_regression"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=0.01))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params,
bagging = FALSE, bagging_R = 100, ensemble = FALSE, feature_selection_method = NULL,
k = NULL, drop_missing_records = FALSE, fill_missing_method = "mean",
scale_data = FALSE, remove_outliers = FALSE, seed = 123, parameter_tuning = FALSE, cv_folds = 10, ensemble_combine_method = NULL)
assert_that(!is.null(results$model))
print(results$predictions[1:10])
assert_that(!('mse' %in% names(results$metrics)), msg = 'MSE is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 6)
data = data_builder('classification')
X = data$X
y = data$y
model_type = "logistic_regression"
models = list(model_type)
model_tuning_params <- list(model_type = list(lambda=0.01))
results <- unifiedModelingToolkit(X = X, y = y, models = models, model_tuning_params = model_tuning_params,
bagging = FALSE, bagging_R = 100, ensemble = FALSE, feature_selection_method = NULL,
k = NULL, drop_missing_records = FALSE, fill_missing_method = "mean",
scale_data = TRUE, remove_outliers = FALSE, seed = 123, parameter_tuning = TRUE, cv_folds = 10, ensemble_combine_method = NULL)
assert_that(!is.null(results$model))
assert_that(length(unique(results$predictions)) == 2)
assert_that(('accuracy' %in% names(results$metrics)), msg = 'Accuracy is not present in metrics')
test_predictions = make_predictions(results$model, head(X))
assert_that('predictions' %in% names(test_predictions) && length(test_predictions$predictions) == 6)
X <- data.frame(matrix(rnorm(100),nrow=20),X6=sample(c('a','b'),20,replace=TRUE))
y <- rnorm(20)
data1 <- data.frame(y,X)
lm(y~.,data=data1)
results <- unifiedModelingToolkit(X = X, y = y, models = list('linear_regression'))
X <- data.frame(matrix(rnorm(100),nrow=20),X6=sample(c('a','b'),20,replace=TRUE))
head(X)
glm(y~.,data=data1)
results <- unifiedModelingToolkit(X = X, y = y, models = list('linear_regression'))
results <- unifiedModelingToolkit(X = X, y = y, models = list('linear_regression'))
head(X)
X <- data.frame(matrix(rnorm(100),nrow=20),X6=sample(c('a','b','c'),20,replace=TRUE))
head(X)
y <- rnorm(20)
data1 <- data.frame(y,X)
lm(y~.,data=data1)
results <- unifiedModelingToolkit(X = X, y = y, models = list('linear_regression'))
results
X <- data.frame(matrix(rnorm(100),nrow=20),X6=sample(c('a','b','c'),20,replace=TRUE))
head(X)
y <- rnorm(20)
data1 <- data.frame(y,X)
lm(y~.,data=data1)
results <- unifiedModelingToolkit(X = X, y = y, models = list('linear_regression'))
X <- data.frame(matrix(rnorm(100),nrow=20),X6=sample(c('a','b','c'),20,replace=TRUE))
y <- c(rep(0,10),rep(1,10))
data1 <- data.frame(y,X)
glm(y~.,data=data1,family='binomial')
results <- unifiedModelingToolkit(X = X, y = y, models = list('logistic_regression'))
X <- data.frame(matrix(rnorm(100),nrow=100),X6=sample(c('a','b'),100,replace=TRUE))
head(X)
y <- rnorm(20)
data1 <- data.frame(y,X)
lm(y~.,data=data1)
results <- unifiedModelingToolkit(X = X, y = y, models = list('linear_regression'))
X <- data.frame(matrix(rnorm(100),nrow=100),X6=sample(c('a','b'),100,replace=TRUE))
head(X)
y <- rnorm(100)
data1 <- data.frame(y,X)
lm(y~.,data=data1)
results <- unifiedModelingToolkit(X = X, y = y, models = list('linear_regression'))
X <- matrix(rnorm(100),nrow=20),X6=sample(c('a','b'),20,replace=TRUE)
y <- c(rep(0,10),rep(1,10))
